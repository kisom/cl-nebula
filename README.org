#+TITLE: The Nebula file server

* Introduction

  *Nebula* is a prototype file store that uses a capability model for
  security and a content-addressable storage scheme for the underlying
  data. This particular version is a development demonstrator that
  aims to vet the core ideas.

  The motivations and general concepts are explained in more depth
  in [[https://kyleisom.net/blog/2015/04/07/nebula/][this blog post]].

** What problem are you trying to solve?

   Users have data: they want to be able to keep track of revisions to
   this data, and they would like to be able to share this data with
   other users. Users would also like the ability to cluster into
   groups to share and collaborate on this data.

   Secondary objectives are to build real world experience designing,
   implementing, and operating capability systems; and to characterise
   the behaviour of capability systems in the real world.

** What are the characteristics of a solution?

   1. Users must be able to upload and retrieve data.
   2. Users must be able to view the history of their data.
   3. Users should be able to share data with other users.
   4. A user should be able to refer to a piece of data as a leaf in a
      history tree, as a node in the tree, or as an isolated snapshot
      with the history information stripped.
   5. Users should have some assurance as to the integrity and
      confidentiality of their data: one user should not be able to
      read another user's file unless permission has been explicitly
      granted *or* unless the other user has their own copy of that
      data.

** Towards a solution

   The pieces of such a solution are described below.

*** Data blobs

   Data is referred to by the SHA-256 hash of the contents of the
   file. For technical reasons, this could be prefixed to reside in
   some directory tree structure. There are two options for this: use
   a prefix (such as the first *n* bytes of the ID, or where each byte
   is a directory. Example:

   Example: given the SHA-256 ID

#+BEGIN_EXAMPLE
000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
#+END_EXAMPLE


   The first solution (with a prefix of 4) yields the path

#+BEGIN_EXAMPLE
0001/02030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
#+END_EXAMPLE

   The second yields

#+BEGIN_EXAMPLE
00/01/02/03/04/05/06/07/08/09/0a/0b/0c/0d/0e/0f/10/11/12/13/14/15/16/17/\
18/19/1a/1b/1c/1d/1e/1f
#+END_EXAMPLE

    From a file system performance perspective, the two options are
    equally difficult to implement, and the latter will provide better
    performance. As data will always be referenced by a SHA-256 hash
    (a constraint that **must** be enforced in the code), this should
    not be a problem.

*** References to blobs

Users directly interacting with blobs presents two problems:

1. Information leakage: if Alice wants to determine if someone already
   has a copy of some data, she attempts to read its SHA-256
   digest. The server will return the data if she has it. This data is
   of no consequence to Alice, as she likely already had a copy of the
   data to produce the hash.
2. Managing data is more difficult: in the case where a user asks the
   server to delete a file that multiple users have, the server has no
   way to determine what other users might have the data. One user can
   then remove data that other users may still wish to
   retain. Alternatively, the server might refuse to delete this data,
   which means users have no way to remove data from the system.

   The solution is to only access blob IDs in the software, and to
   provide users with UUIDs to reference their data. A UUID contains

   + the ID
   + the referenced object ID (see below, this may be a SHA-256 ID or
     another UUID)
   + metadata
   + the parent UUID
   + UUIDs of any children

   In order to provide useful access control, a reference may be
   a *proxy* reference: that is, it may refer to another blob
   reference. This means that a user can grant revocable access to the
   data *without* jeopardizing their own access.

   Therefore, to know an ID is to have access to that ID. For this
   reason, users can only see the metadata and none of the IDs. The
   system needs an API that can traverse the history tree without
   exposing these IDs to users. Proxy objects either need to be
   presented with no history information (empty parent and children),
   or the entire history needs to be proxied. Similarly, a revocation
   API needs to be able to take into account that the entire history
   tree may be proxied.

   This data must be persisted, such as a via a database.

   This reference is named an *entry*. This reference is the only
   interface users have with blobs. A user should never see the target
   of an entry, nor should they be able to determine whether an ID was
   proxied.

*** Named histories

    Constantly referring to a UUID for file revisions is something
    that users will find awkward. A useful abstraction is a named
    history: presenting a single reference to a history tree that
    always provides the newest copy of some data, while still allowing
    users to traverse the history. This abstraction needs to pair some
    notion of the owner with a name of their choosing; this pairing is
    termed a *file*. Writing to a file creates a new entry with the
    parent set to the file's current reference, and the file's
    reference is updated to the new entry's ID.

    This might best be handled by the application using Nebula, which
    can translated the entry to an appropriate storage metaphor.

*** Users

    Users will be identified by a UUID, as will collections of users
    (termed a *group*). This allows groups and users to be
    interchangeable.

    *Challenge*: how to deal with removing a user from a group? To know
    an ID is to have access to the ID, so new IDs will need to be
    generated for each object owned by a group; this change will need to
    be communicated to the group. Groups are not granular at this time:
    access to a group ID means all users can read or write entries and
    files. Group leadership will probably belong to a single user. This a
    subject that should be considered for revision in future.

    The subject of groups and user management is also probably best
    handled by the application using Nebula, allowing them to
    translate the idea of an owner to an appropriate metaphor.

** A demo use case

   A demo of the Nebula system would be to build an HTTP front end
   that uses [[https://codemirror.org/][Codemirror]] to implement a
   collaborative editor.

** Sync

   At some point it would be advantageous to sync data. Armstrong
   [[http://joearms.github.io/2015/03/12/The_web_of_names.html][proposes]] the use of a DHT. However, implementing sync in this
   manner means that any participating node has access to all the
   blobs where no guarantee is made that peers are securing this data;
   this presents a large hole for data leakage. Participating
   nodes **must** have some sort of authentication. The most
   straight-forward mechanism for this is to communicate over an
   interface such as [[https://kyleisom.net/projects/schannel/][schannel]] with mutual authentication. This brings
   the complexity of requiring a signature authority trusted by all
   users. A synchronisation mechanism must operate in a hostile
   environment:

   + At the core, user data *must* be protected: just as users expect
     their data to remain secure on the single node system, so too should
     their data be secured across all nodes.
   + A participant should expect that some participants are actively
     trying to exploit data leakage.
   + Participants must have strong mutual authentication, which implies
     strong identity. Nodes may be pseudonymous, but they cannot be fully
     anonymous. Peer reputation is a necessity.
   + Communications **must** occur over a secure channel (see
     *Cryptography Engineering* or `schannel`).
   + Alternate (not schannel) alternatives should be explored. One
     alternative is hosts identified by a UUID and using remote
     attestation or another form of TPM-based
     authentication. Particularly interesting would be decentralised
     authentication and attestation, but it is difficult to see how trust
     could be bootstrapped this way.

* Prerequisites

  You will need a Lisp interpreter and [[http://quicklisp.org/][Quicklisp]].

  You will need a Postgres database running; the credentials should be
  in an alist in the file ~$HOME/.nebula.lisp~. This alist should look
  like

#+BEGIN_EXAMPLE
;;; Example names taken from the postmodern docs.
((:DB-HOST "localhost")    ;; hostname of database
 (:DB-NAME "testdb")       ;; database name
 (:DB-PASS "surveiller")   ;; password
 (:DB-PORT 5432)           ;; port
 (:DB-USER "focault"))     ;; username
#+END_EXAMPLE

  As it is expected that this will run on ~localhost~, no SSL
  connections are used.

  The database will need to exist and be writeable by the user.

  This code should be stored in the Quicklisp ~local-projects~
  directory so that it may be loaded with Quicklisp.

* Using

  Nebula is a library for exploring this idea of a filestore.

  It should be cloned into the Quicklisp local-projects directory;
  then it may included as a dependency or loaded with
  ~(ql:quickload :nebula)~.

  Once the credentials file is created, the package should be
  initialised with ~(nebula:initialize)~. It will create any tables it
  needs on startup.

* The API
*** initialize

#+BEGIN_EXAMPLE
Lambda-list: (&KEY CRED-PATH STORE-PATH)
#+END_EXAMPLE

Conducts the necessary setup to begin using the filestore. If
~cred-path~ is not ~nil~, it is used as the path to the Postgres
credentials file (described in the setup section). If ~store-path~
is not ~nil~, it is used as the path to the blob store.

*** retrieve

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Type: (FUNCTION (STRING) (OR (SIMPLE-ARRAY (UNSIGNED-BYTE 8)) NULL))
#+END_EXAMPLE

Given a UUID, follow its targets all the way to the underlying
blob. If the UUID doesn't exist or can't be followed for some reason,
it returns ~nil~; otherwise, it returns a byte array containing
underlying blob.

*** store

#+BEGIN_EXAMPLE
Lambda-list: (DATA &KEY (PARENT NIL))
Type: (FUNCTION ((OR STRING (SIMPLE-ARRAY (UNSIGNED-BYTE 8)))
       &KEY (:PARENT (OR STRING NULL)))
       (OR NULL STRING))
#+END_EXAMPLE

Store some data, possibly under a parent entry. If ~parent~ is
not ~nil~ and is a valid UUID, the resulting entry will use ~parent~
as its parent. Otherwise, an entry with no history will be created.

*** expunge

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Derived type: (FUNCTION (STRING) (BOOLEAN))
#+END_EXAMPLE

Expunge the entry named by UUID from the system, garbage collecting as
required. If any other entries have this entry as a parent, their
parent entry will be cleared. Any proxy entries pointing to this one
will be removed.

*** info

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Derived type: (FUNCTION (STRING) (LIST))
#+END_EXAMPLE

Returns an alist with the keys ~:id~, ~:created~, ~:size~, and
~:parent~, filled with the relevant information for the entry named by
~uuid~.

*** lineage

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Derived type: (FUNCTION (STRING) CONS)
#+END_EXAMPLE

Return a list of entry IDs of this entry's lineage. The ~car~ of this
list will be current entry, and the last element will be the last
parent.

*** proxy

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Derived type: (FUNCTION (STRING) (OR STRING NULL)
#+END_EXAMPLE

Proxy a single entry, removing its history. It returns the identifier
for this proxied entry.

*** proxy-all

#+BEGIN_EXAMPLE
Lambda-list: (UUID)
Derived type: (FUNCTION (STRING) CONS)
#+END_EXAMPLE

Proxies an entire lineage for an entry. The history is preserved, though
replaced with the proxied equivalent.

* TODOs, thoughts, and considerations

  + Work on additional front-ends
    + CLOS persistence store?
    + 9P interface?
    + An editor for collaborating and working on files over the
      network? Maybe an object-capable editor?
  + Blobs are stored insecurely; does this matter? What's a good way
    to fix this?
  + Support for trees of history
  + Resource restrictions
    + Quotas
    + File size restrictions
  + The backends could use some looking at
    + Support for multiple backends
    + Options like [[https://common-lisp.net/project/elephant/][elephant]] or [[http://cl-www.msi.co.jp/projects/manardb/][manardb]]
    + Custom CLOS persistence store
  + Nebula was designed for use on a single host. What would it look
    like to distribute this?
    + Does it make sense to deal with distribution at this level,
      or is better to do it at the interface level?
    + What topologies make senes?
    + Do all the nodes share binary data, or just tell each other
      which blobs and entries they have?
    + Namespacing?
  + The documentation could better introduce the ideas and concepts
    behind this system.

